{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arlX8FeMtabc"
      },
      "source": [
        "# Langchain Crash Course by BhaveshðŸ¦œ\n",
        "[**Link to my YouTube Channel**](https://www.youtube.com/BhaveshBhatt8791?sub_confirmation=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installs"
      ],
      "metadata": {
        "id": "SodUZXmugnZA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7tOFd1ttabd"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q langchain\n",
        "!pip install -q openai\n",
        "!pip install -q huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "hj2WQa9whw7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "with open('API_Keys.json') as f:\n",
        "    data = json.load(f)\n",
        "     \n",
        "os.environ['OPENAI_API_KEY'] = data[\"openai_key\"]\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = data[\"hf_key\"]"
      ],
      "metadata": {
        "id": "3uuYhNQXfdmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Unified Interface"
      ],
      "metadata": {
        "id": "E5XUDmJRhGGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain.llms as list_of_llms"
      ],
      "metadata": {
        "id": "-YOeDU-ziA0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir(list_of_llms)"
      ],
      "metadata": {
        "id": "-IkH4O4fiISQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22588e73-b640-461a-8216-8815207d12c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['AI21',\n",
              " 'AlephAlpha',\n",
              " 'Anthropic',\n",
              " 'Anyscale',\n",
              " 'Aviary',\n",
              " 'AzureOpenAI',\n",
              " 'Banana',\n",
              " 'BaseLLM',\n",
              " 'Baseten',\n",
              " 'Beam',\n",
              " 'Bedrock',\n",
              " 'CTransformers',\n",
              " 'CerebriumAI',\n",
              " 'Cohere',\n",
              " 'Databricks',\n",
              " 'DeepInfra',\n",
              " 'Dict',\n",
              " 'FakeListLLM',\n",
              " 'ForefrontAI',\n",
              " 'GPT4All',\n",
              " 'GooglePalm',\n",
              " 'GooseAI',\n",
              " 'HuggingFaceEndpoint',\n",
              " 'HuggingFaceHub',\n",
              " 'HuggingFacePipeline',\n",
              " 'HuggingFaceTextGenInference',\n",
              " 'HumanInputLLM',\n",
              " 'LlamaCpp',\n",
              " 'Modal',\n",
              " 'MosaicML',\n",
              " 'NLPCloud',\n",
              " 'OpenAI',\n",
              " 'OpenAIChat',\n",
              " 'OpenLM',\n",
              " 'Petals',\n",
              " 'PipelineAI',\n",
              " 'PredictionGuard',\n",
              " 'PromptLayerOpenAI',\n",
              " 'PromptLayerOpenAIChat',\n",
              " 'RWKV',\n",
              " 'Replicate',\n",
              " 'SagemakerEndpoint',\n",
              " 'SelfHostedHuggingFaceLLM',\n",
              " 'SelfHostedPipeline',\n",
              " 'StochasticAI',\n",
              " 'Type',\n",
              " 'VertexAI',\n",
              " 'Writer',\n",
              " '__all__',\n",
              " '__annotations__',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " 'ai21',\n",
              " 'aleph_alpha',\n",
              " 'anthropic',\n",
              " 'anyscale',\n",
              " 'aviary',\n",
              " 'bananadev',\n",
              " 'base',\n",
              " 'baseten',\n",
              " 'beam',\n",
              " 'bedrock',\n",
              " 'cerebriumai',\n",
              " 'cohere',\n",
              " 'ctransformers',\n",
              " 'databricks',\n",
              " 'deepinfra',\n",
              " 'fake',\n",
              " 'forefrontai',\n",
              " 'google_palm',\n",
              " 'gooseai',\n",
              " 'gpt4all',\n",
              " 'huggingface_endpoint',\n",
              " 'huggingface_hub',\n",
              " 'huggingface_pipeline',\n",
              " 'huggingface_text_gen_inference',\n",
              " 'human',\n",
              " 'llamacpp',\n",
              " 'loading',\n",
              " 'modal',\n",
              " 'mosaicml',\n",
              " 'nlpcloud',\n",
              " 'openai',\n",
              " 'openlm',\n",
              " 'petals',\n",
              " 'pipelineai',\n",
              " 'predictionguard',\n",
              " 'promptlayer_openai',\n",
              " 'replicate',\n",
              " 'rwkv',\n",
              " 'sagemaker_endpoint',\n",
              " 'self_hosted',\n",
              " 'self_hosted_hugging_face',\n",
              " 'stochasticai',\n",
              " 'type_to_cls_dict',\n",
              " 'utils',\n",
              " 'vertexai',\n",
              " 'writer']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI, HuggingFaceHub"
      ],
      "metadata": {
        "id": "28xxYpLhiIVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_1 = \"Who is Micheal Jackson?\""
      ],
      "metadata": {
        "id": "aQ9Rw5QJlqKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_1 = OpenAI(model_name='text-davinci-003')"
      ],
      "metadata": {
        "id": "HzljKgLNiA91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm1_output = llm_1(text_1)\n",
        "print(llm1_output)"
      ],
      "metadata": {
        "id": "ycUPy44hfdvy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f9b5a94-0210-41eb-aa82-841e82178c3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Michael Jackson was an American singer, songwriter, and dancer. He is considered one of the most influential entertainers of all time and is known for his iconic music videos, dance moves, and signature style. He was dubbed the \"King of Pop\" and his music has influenced many artists across a variety of genres.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_2 = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-base\",\n",
        "    model_kwargs={\"temperature\":0.9, \"max_length\": 64},\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "llm2_output = llm_2(text_1)\n",
        "\n",
        "print(llm2_output)"
      ],
      "metadata": {
        "id": "1Ma9HD7Elu8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eec6a77-92f4-4a54-8a69-dc9d9b5e7924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "philanthropist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kFOXuwXrif1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Prompt Templates & Chains"
      ],
      "metadata": {
        "id": "bKGBTZAopADd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "inp_template = \"\"\"Question: {input_question}\n",
        "\n",
        "Please think step by step and reach at the answer.\n",
        "\n",
        "Answer: \"\"\""
      ],
      "metadata": {
        "id": "FZJ0129ZlDli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(template=inp_template, \n",
        "                        input_variables=[\"input_question\"])"
      ],
      "metadata": {
        "id": "UXTFJV7Tlinx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(prompt)"
      ],
      "metadata": {
        "id": "sR_yhhyfrm6x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9c9a8e3-eddb-4c13-c393-237318da8cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain.prompts.prompt.PromptTemplate"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "id": "mhgK-vEUrnBG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20093e74-007b-4449-b8a7-b074a40d7d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['input_question'], output_parser=None, partial_variables={}, template='Question: {input_question}\\n\\nPlease think step by step and reach at the answer.\\n\\nAnswer: ', template_format='f-string', validate_template=True)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.format(input_question = \"Can Generative AI takeaway automation based jobs?\")"
      ],
      "metadata": {
        "id": "mony4uLVrycx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "401c864d-edab-47da-b2eb-aeb9603b59ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Question: Can Generative AI takeaway automation based jobs?\\n\\nPlease think step by step and reach at the answer.\\n\\nAnswer: '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm_1, prompt=prompt)\n",
        "\n",
        "print(chain.run(\"Can Generative AI takeaway automation based jobs?\"))"
      ],
      "metadata": {
        "id": "xMoHKrgJryfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8388211a-31f6-4242-93e8-f87f2e627723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1: Generative AI is a type of artificial intelligence that is used to create new and innovative ideas or content.\n",
            "\n",
            "Step 2: Automation based jobs refer to jobs that are replaced by machines or robots. \n",
            "\n",
            "Step 3: Generative AI does not directly replace automation based jobs, but it can be used to increase efficiency in the production process which could ultimately lead to the automation of certain jobs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Combining Chains"
      ],
      "metadata": {
        "id": "0bRLOa2fs535"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_1 = PromptTemplate(\n",
        "            template=\"Who is the first President of {country}?\",\n",
        "            input_variables=[\"country\"],\n",
        "        )\n",
        "\n",
        "chain_1 = LLMChain(llm = llm_1, prompt = prompt_1)\n",
        "print(chain_1.run(\"India\"))"
      ],
      "metadata": {
        "id": "VRGD0pIusBNl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "553a3229-6d60-4670-a849-dad1c5e18c36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The first President of India was Dr. Rajendra Prasad.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_2 = PromptTemplate(\n",
        "    input_variables=[\"input_president\"],\n",
        "    template=\"When was {input_president} born?\",\n",
        ")\n",
        "\n",
        "chain_2 = LLMChain(llm = llm_1, prompt = prompt_2)\n",
        "print(chain_2.run(\"Dr. Rajendra Prasad\"))"
      ],
      "metadata": {
        "id": "CR-wfHqatCZ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ad2607-eb49-4b33-8bed-38d8229b7d21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Dr. Rajendra Prasad was born on 3 December 1884.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "overall_chain = SimpleSequentialChain(chains=[chain_1, \n",
        "                                              chain_2], \n",
        "                                      verbose=True)\n",
        "\n",
        "prez_details = overall_chain.run(\"Australia\")\n",
        "print(prez_details)"
      ],
      "metadata": {
        "id": "TL0oGBd_sBQs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a055ae4-3c08-4345-e692-91f9a4ed5c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m\n",
            "\n",
            "The first President of Australia was Sir William Deane, who served from 1996 to 1997.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "\n",
            "Sir William Deane was born on 4 August 1931.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "Sir William Deane was born on 4 August 1931.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Agents and Tools"
      ],
      "metadata": {
        "id": "Iya7HU0T3OaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools, initialize_agent"
      ],
      "metadata": {
        "id": "xNqRDPnbsBWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = load_tools(['llm-math'], llm=llm_1)"
      ],
      "metadata": {
        "id": "mz9heiNA3R9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_3 = PromptTemplate(\n",
        "            template=\"\"\"\n",
        "            How many world cups has {cricketer} won & what is the result when this number is raised to the 0.67 power?\n",
        "            \"\"\",\n",
        "            input_variables=[\"cricketer\"],\n",
        "        )\n",
        "\n",
        "chain_3 = LLMChain(llm = llm_1, \n",
        "                   prompt = prompt_3,\n",
        "                   verbose=True)"
      ],
      "metadata": {
        "id": "PYgcyLlV3SFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_3.run(\"Ricky Ponting\")"
      ],
      "metadata": {
        "id": "5dhnZZnq7EIz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "6582a940-a93d-444b-d795-8f262be27634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "            How many world cups has Ricky Ponting won & what is the result when this number is raised to the 0.67 power?\n",
            "            \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nRicky Ponting has won two World Cups. When this number is raised to the 0.67 power, the result is approximately 1.26.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Memory"
      ],
      "metadata": {
        "id": "5yc0ocER7uJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_1(\"Hello, My name is Bhavesh\"))"
      ],
      "metadata": {
        "id": "TCxtj3H9liqv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7746e4a8-8481-435e-d2f4-8a3586564f2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Nice to meet you, Bhavesh. It's nice to meet you too!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_1(\"What is the capital of Sri Lanka?\"))"
      ],
      "metadata": {
        "id": "Wywa4V_Q7rew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8b20c9e-3acb-41f1-8fb6-65aff8994dc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The capital of Sri Lanka is Sri Jayawardenepura Kotte.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_1(\"What is the value of 121212 + 412312?\"))"
      ],
      "metadata": {
        "id": "jX-Sjbly7rhq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78b40fb2-7715-47f1-d1cf-79b33c494db7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "533,524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_1(\"What is my name?\"))"
      ],
      "metadata": {
        "id": "-0WvV3jT7rkM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95cb4c71-dbbb-4b97-e224-efdaac638e4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "That depends on who you are asking!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, ConversationChain\n",
        "\n",
        "chatbot = ConversationChain(llm = llm_1, \n",
        "                            verbose=True)"
      ],
      "metadata": {
        "id": "edrsAUtJ8jUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot.predict(input=\"Hello, My name is Bhavesh\")"
      ],
      "metadata": {
        "id": "9XfPN_Bz8jX6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "e4e29051-1622-490a-d11c-8a585fb0f905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hello, My name is Bhavesh\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hi Bhavesh, my name is AI. It's nice to meet you. What can I help you with?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot.predict(input=\"What is the capital of Sri Lanka?\")"
      ],
      "metadata": {
        "id": "cr0LtR2Y8jcT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "e83605c2-76ba-413e-fd96-b0bd9fec1b85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hello, My name is Bhavesh\n",
            "AI:  Hi Bhavesh, my name is AI. It's nice to meet you. What can I help you with?\n",
            "Human: What is the capital of Sri Lanka?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The capital of Sri Lanka is Colombo. It is located on the west coast of the island. It is the largest and most populous city in Sri Lanka with an estimated population of over seven million people.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot.predict(input=\"What is the value of 121212 + 412312?\")"
      ],
      "metadata": {
        "id": "3G5sMPmL8yS0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "313b0fe2-d8d3-48e2-8983-39f533d3e491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hello, My name is Bhavesh\n",
            "AI:  Hi Bhavesh, my name is AI. It's nice to meet you. What can I help you with?\n",
            "Human: What is the capital of Sri Lanka?\n",
            "AI:  The capital of Sri Lanka is Colombo. It is located on the west coast of the island. It is the largest and most populous city in Sri Lanka with an estimated population of over seven million people.\n",
            "Human: What is the value of 121212 + 412312?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The value of 121212 + 412312 is 533524.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "donE4_Fstabg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "d640ef29-df78-405f-d6a4-3694b56a1ab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hello, My name is Bhavesh\n",
            "AI:  Hi Bhavesh, my name is AI. It's nice to meet you. What can I help you with?\n",
            "Human: What is the capital of Sri Lanka?\n",
            "AI:  The capital of Sri Lanka is Colombo. It is located on the west coast of the island. It is the largest and most populous city in Sri Lanka with an estimated population of over seven million people.\n",
            "Human: What is the value of 121212 + 412312?\n",
            "AI:  The value of 121212 + 412312 is 533524.\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Your name is Bhavesh.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "chatbot.predict(input=\"What is my name?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "THr5t-yx8nGj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env_twitter",
      "language": "python",
      "name": "env_twitter"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}